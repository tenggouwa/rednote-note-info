import time
import json
from bs4 import BeautifulSoup
import undetected_chromedriver as uc

def extract_note_detail(browser, url):
    browser.get(url)
    time.sleep(2)

    soup = BeautifulSoup(browser.page_source, 'html.parser')

    # æ ‡é¢˜
    title_tag = soup.find('div', id='detail-title')
    title = title_tag.get_text(strip=True) if title_tag else ''

    # æè¿°
    desc_tag = soup.find('div', id='detail-desc')
    description = desc_tag.get_text(strip=True) if desc_tag else ''

    # å‘å¸ƒæ—¶é—´
    date_tag = soup.find('span', class_='date')
    publish_date = date_tag.get_text(strip=True) if date_tag else ''

    # å›¾ç‰‡
    img_tags = soup.find_all('img', class_='note-slider-img')
    image_urls = [img.get('src') for img in img_tags if img.get('src')]

    # ğŸ‘ ç‚¹èµæ•°
    like_span = soup.select_one('span.like-wrapper span.count')
    likes = like_span.get_text(strip=True) if like_span else '0'

    # â­ æ”¶è—æ•°
    collect_span = soup.select_one('span.collect-wrapper span.count')
    collects = collect_span.get_text(strip=True) if collect_span else '0'

    # ğŸ’¬ è¯„è®ºæ•°
    comment_span = soup.select_one('span.chat-wrapper span.count')
    comments = comment_span.get_text(strip=True) if comment_span else '0'

    return {
        'detail_title': title,
        'description': description,
        'publish_date': publish_date,
        'images': image_urls,
        'likes': likes,
        'collects': collects,
        'comments': comments
    }

def scrape_notes(keyword='éœ²è¥', max_scroll=6, max_notes=10):
    url = f'https://www.xiaohongshu.com/search_result?keyword={keyword}&source=web_explore_feed'
    
    options = uc.ChromeOptions()
    options.add_argument("--start-maximized")

    browser = uc.Chrome(options=options)
    browser.get(url)

    print("ğŸ” è¯·åœ¨å¼¹å‡ºçš„æµè§ˆå™¨ä¸­ç™»å½•å°çº¢ä¹¦ï¼Œç™»å½•å®ŒæˆåæŒ‰å›è½¦ç»§ç»­...")
    input("âœ… ç™»å½•å®ŒæˆåæŒ‰å›è½¦...")

    # æ»šåŠ¨åŠ è½½é¡µé¢å†…å®¹
    for _ in range(max_scroll):
        browser.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)

    soup = BeautifulSoup(browser.page_source, 'html.parser')
    cards = soup.find_all('section', class_='note-item')

    print(f"ğŸ“¦ æœç´¢é¡µå…±æ‰¾åˆ° {len(cards)} æ¡å†…å®¹")

    results = []

    for idx, card in enumerate(cards):
        if len(results) >= max_notes:
            break
        try:
            # åˆ—è¡¨é¡µï¼šæå–æ ‡é¢˜ã€å°é¢ã€ä½œè€…ã€é“¾æ¥
            title_tag = card.find('a', class_='title')
            title = title_tag.get_text(strip=True) if title_tag else ''

            img_tag = card.find('a', class_='cover').find('img')
            image = img_tag['src'] if img_tag else ''

            author_span = card.find('span', class_='name')
            author = author_span.get_text(strip=True) if author_span else ''

            link_tag = card.find('a', class_='cover')
            relative_url = link_tag['href'] if link_tag else ''
            link = 'https://www.xiaohongshu.com' + relative_url

            note = {
                'title': title,
                'image': image,
                'author': author,
                'link': link
            }

            print(f"â¡ï¸ æŠ“å–è¯¦æƒ…é¡µ {idx + 1}: {title}")
            detail = extract_note_detail(browser, link)
            note.update(detail)

            results.append(note)

        except Exception as e:
            print(f"âŒ æŠ“å–å¤±è´¥: {e}")
            continue

    browser.quit()

    return results

if __name__ == "__main__":
    data = scrape_notes(keyword='éœ²è¥', max_scroll=20, max_notes=150)

    with open('xhs_camping_data.json', 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"âœ… å…±æŠ“å– {len(data)} æ¡ç¬”è®°ï¼Œå·²ä¿å­˜è‡³ xhs_camping_data.json")