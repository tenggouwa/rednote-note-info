import time
import json
from bs4 import BeautifulSoup
import undetected_chromedriver as uc

def extract_note_detail(browser, url):
    browser.get(url)
    time.sleep(5)

    soup = BeautifulSoup(browser.page_source, 'html.parser')

    # æ ‡é¢˜
    title_tag = soup.find('div', id='detail-title')
    title = title_tag.get_text(strip=True) if title_tag else ''

    # æè¿°
    desc_tag = soup.find('div', id='detail-desc')
    description = desc_tag.get_text(strip=True) if desc_tag else ''

    # å‘å¸ƒæ—¶é—´
    date_tag = soup.find('span', class_='date')
    publish_date = date_tag.get_text(strip=True) if date_tag else ''

    # å›¾ç‰‡
    img_tags = soup.find_all('img', class_='note-slider-img')
    image_urls = [img.get('src') for img in img_tags if img.get('src')]

    # ğŸ‘ ç‚¹èµæ•°
    like_span = soup.select_one('span.like-wrapper span.count')
    likes = like_span.get_text(strip=True) if like_span else '0'

    # â­ æ”¶è—æ•°
    collect_span = soup.select_one('span.collect-wrapper span.count')
    collects = collect_span.get_text(strip=True) if collect_span else '0'

    # ğŸ’¬ è¯„è®ºæ•°
    comment_span = soup.select_one('span.chat-wrapper span.count')
    comments = comment_span.get_text(strip=True) if comment_span else '0'

    return {
        'detail_title': title,
        'description': description,
        'publish_date': publish_date,
        'images': image_urls,
        'likes': likes,
        'collects': collects,
        'comments': comments
    }

def scrape_notes(keyword='éœ²è¥', max_scroll=6, max_notes=10):
    url = f'https://www.xiaohongshu.com/search_result?keyword={keyword}&source=web_explore_feed'
    
    options = uc.ChromeOptions()
    options.add_argument("--start-maximized")

    browser = uc.Chrome(options=options)
    browser.get(url)

    print("ğŸ” è¯·åœ¨å¼¹å‡ºçš„æµè§ˆå™¨ä¸­ç™»å½•å°çº¢ä¹¦ï¼Œç™»å½•å®ŒæˆåæŒ‰å›è½¦ç»§ç»­...")
    input("âœ… ç™»å½•å®ŒæˆåæŒ‰å›è½¦...")

    results = []
    seen_links = set()

    # æ»šåŠ¨åŠ è½½é¡µé¢å†…å®¹
    for scroll_count in range(max_scroll):
        browser.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(5)
        # æ¯æ¬¡æ»šåŠ¨åï¼Œå¢é‡æ›´æ–°æ•°æ®
        soup = BeautifulSoup(browser.page_source, 'html.parser')
        cards = soup.find_all('section', class_='note-item')
        print(f"ğŸ”„ æ»šåŠ¨ {scroll_count + 1}/{max_scroll} æ¬¡åï¼Œé¡µé¢ä¸Šå…±æœ‰ {len(cards)} æ¡å†…å®¹")

        for card in cards:
            try:
                # åˆ—è¡¨é¡µï¼šæå–æ ‡é¢˜ã€å°é¢ã€ä½œè€…ã€é“¾æ¥
                title_tag = card.find('a', class_='title')
                title = title_tag.get_text(strip=True) if title_tag else ''

                img_tag = card.find('a', class_='cover').find('img')
                image = img_tag['src'] if img_tag else ''

                author_span = card.find('span', class_='name')
                author = author_span.get_text(strip=True) if author_span else ''

                link_tag = card.find('a', class_='cover')
                relative_url = link_tag['href'] if link_tag else ''
                link = 'https://www.xiaohongshu.com' + relative_url

                if link in seen_links:
                    continue

                note = {
                    'title': title,
                    'image': image,
                    'author': author,
                    'link': link
                }

                print(f"â¡ï¸ æŠ“å–è¯¦æƒ…é¡µ: {title}")
                detail = extract_note_detail(browser, link)
                note.update(detail)

                results.append(note)
                seen_links.add(link)

                if len(results) >= max_notes:
                    break

            except Exception as e:
                print(f"âŒ æŠ“å–å¤±è´¥: {e}")
                continue

        if len(results) >= max_notes:
            break

    browser.quit()

    return results

if __name__ == "__main__":
    data = scrape_notes(keyword='éœ²è¥', max_scroll=40, max_notes=150)

    with open('xhs_camping_data.json', 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"âœ… å…±æŠ“å– {len(data)} æ¡ç¬”è®°ï¼Œå·²ä¿å­˜è‡³ xhs_camping_data.json")